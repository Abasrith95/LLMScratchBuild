{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "404faa3740514de591a731e4cf69222b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abeed0a7d280450b815793137a0364c8",
              "IPY_MODEL_8733e10ce3fd4f21a21dd8ad98e7d809"
            ],
            "layout": "IPY_MODEL_35ebd5725e264246882a4d49281c763d"
          }
        },
        "abeed0a7d280450b815793137a0364c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b4ab10806c40909c51ad39e1d25d28",
            "placeholder": "​",
            "style": "IPY_MODEL_3d70e776f25048aba292bd1e2f1b0c4c",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "8733e10ce3fd4f21a21dd8ad98e7d809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1211c274b3d4f33aae65cd4a0737f29",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8c6e5c40b0646f1a891900ff3b1e460",
            "value": 1
          }
        },
        "35ebd5725e264246882a4d49281c763d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1b4ab10806c40909c51ad39e1d25d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d70e776f25048aba292bd1e2f1b0c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1211c274b3d4f33aae65cd4a0737f29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8c6e5c40b0646f1a891900ff3b1e460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install rotary-embedding-torch\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "ZMmsjfjZSzXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a8d2ef-b5d1-4f90-e1ef-bb1c7dd66121"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.0.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
            "Requirement already satisfied: beartype in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (0.18.5)\n",
            "Requirement already satisfied: einops>=0.7 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (0.8.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->rotary-embedding-torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->rotary-embedding-torch) (1.3.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install datasets\n",
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"Skylion007/openwebtext\", split=\"train\")"
      ],
      "metadata": {
        "id": "kIKQta0mMFGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rotary-embedding-torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpQLKME-Y_3T",
        "outputId": "306b514f-f92e-4669-8688-91b4083cde8e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
            "Requirement already satisfied: beartype in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (0.18.5)\n",
            "Requirement already satisfied: einops>=0.7 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (0.8.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->rotary-embedding-torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->rotary-embedding-torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->rotary-embedding-torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qv0KJ85JO7Kg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import wandb\n",
        "from typing import Tuple\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import tiktoken\n",
        "import gc\n",
        "\n",
        "import inspect\n",
        "from rotary_embedding_torch import RotaryEmbedding\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tiktoken import get_encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "  batch_size: int = 5\n",
        "  block_size: int = 1024\n",
        "  vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "\n",
        "  dim: int = 512\n",
        "  n_layers: int = 8\n",
        "  n_heads: int = 8\n",
        "  max_seq_len: int = 512\n",
        "  layer_norm_eps: float = 1e-6\n",
        "  dropout: float = 0.0\n",
        "  hidden_dim: int = None\n",
        "  n_embd: int = 1024\n",
        "  multiple_of: int = 32\n",
        "  rope_dim: int = 64\n",
        "  bias: bool = True\n",
        "\n",
        "  weight_decay = 1e-1\n",
        "  betas = (0.9, 0.99)\n",
        "  eval_iters = 50\n",
        "  master_process = True\n",
        "  warmup_iters = 0\n",
        "  learning_rate = 0.001\n",
        "  lr_decay_iters = 150\n",
        "  min_lr = 6e-5\n",
        "  wandb_log = True\n",
        "  wandb_project = 'HW5'\n",
        "  wandb_run_name = 'Pretrain_29_4_v1'\n",
        "  decay_lr = True\n",
        "  eval_interval = 100\n",
        "  eval_only = False\n",
        "  grad_clip = 1.0\n",
        "  max_iters = 5000\n",
        "  gradient_accumulation_steps = 2\n",
        "  save_checkpoint_iters = 100\n",
        "  log_interval = 5"
      ],
      "metadata": {
        "id": "rGdAMscDACf_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YEDio1FwOyZ",
        "outputId": "c20f2ff7-b3f2-42b7-95ca-a07dec474cfa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May  3 03:29:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = ModelConfig()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'"
      ],
      "metadata": {
        "id": "WIa-13E7Cd9X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = '/content/pretrained_model'"
      ],
      "metadata": {
        "id": "VaWkzSVHqaKc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_offset = 0\n",
        "\n",
        "# for key, value in config.items():\n",
        "#     globals()[key] = value\n",
        "\n",
        "tokens_per_iter = config.gradient_accumulation_steps * config.batch_size * config.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "device = 'cuda'\n",
        "# config['device'] = device\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "# print(dtype)\n",
        "# config['dtype'] = dtype\n",
        "\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "# ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "# ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "data_dir = '/content/data'\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "# train_data = np.memmap(os.path.join(data_dir, 'train.bin'), mode='r')\n",
        "# Use only 70% of the training data.\n",
        "train_data = train_data[:int(0.7*len(train_data))]\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "# val_data = np.memmap(os.path.join(data_dir, 'val.bin'), mode='r')\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - config.block_size, (config.batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+config.block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+config.block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XP48eIVQPLXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66f98dd-5139-4e86-e543-0478358d662c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 10,240\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        \"\"\"\n",
        "        Initialize the RMSNorm normalization layer.\n",
        "        Args:\n",
        "            dim (int): The dimension of the input tensor.\n",
        "            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
        "        Attributes:\n",
        "            eps (float): A small value added to the denominator for numerical stability.\n",
        "            weight (nn.Parameter): Learnable scaling parameter.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        \"\"\"\n",
        "        Apply the RMSNorm normalization to the input tensor.\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor.\n",
        "        Returns:\n",
        "            torch.Tensor: The normalized tensor.\n",
        "        \"\"\"\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the RMSNorm layer.\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor.\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor after applying RMSNorm.\n",
        "        \"\"\"\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):\n",
        "        super().__init__()\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = 4 * dim\n",
        "            hidden_dim = int(2 * hidden_dim / 3)\n",
        "            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def SwiGLU(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        '''\n",
        "        Compute the SwiGLU activation function (see Section 2 in\n",
        "        https://arxiv.org/abs/2204.02311\n",
        "        '''\n",
        "        return F.silu(self.w1(x)) * self.w3(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.w2(self.SwiGLU(x)))\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_heads == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_heads\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.rotary = RotaryEmbedding(config.rope_dim)\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # apply RoPE, see https://arxiv.org/abs/2104.09864\n",
        "        k = self.rotary.rotate_queries_or_keys(k)\n",
        "        q = self.rotary.rotate_queries_or_keys(q)\n",
        "\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.rn_1 = RMSNorm(config.n_embd, eps=config.layer_norm_eps)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.rn_2 = RMSNorm(config.n_embd, eps=config.layer_norm_eps)\n",
        "        self.mlp = FeedForward(config.n_embd, config.hidden_dim, config.multiple_of, config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.rn_1(x))\n",
        "        x = x + self.mlp(self.rn_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
        "            ln_f = RMSNorm(config.n_embd, eps=config.layer_norm_eps)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layers))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -config.block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "ym_VwXGO5sNg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = GPT(n_layer= config.n_layers, n_head=config.n_heads, n_embd=config.n_embd, block_size=config.block_size,\n",
        "#                   bias=config.bias, dropout= config.dropout)\n",
        "# m= model.to(device)"
      ],
      "metadata": {
        "id": "fsdHjYX-3MF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(config)\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "Bzi4AznqC0s3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGfYpI-ADZPE",
        "outputId": "b60f9fc2-eeed-4b9d-a191-104aae73c937"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "152.749312 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"/content/pretrained_model/checkpoint_iter_3600_3_99_v3.pth\")\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuYHI4oSTPdu",
        "outputId": "ad73bf71-b1f8-4347-bba9-d17113d5c48f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, iter_num, filename='checkpoint.pth'):\n",
        "    torch.save({\n",
        "        'iter_num': iter_num,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, filename)\n",
        "    print(f\"Checkpoint saved at iteration {iter_num}\")"
      ],
      "metadata": {
        "id": "ZoLf8QvOdw9W"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "# scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# optimizer\n",
        "# changed_lr = 6e-5\n",
        "optimizer = model.configure_optimizers(config.weight_decay, config.learning_rate, config.betas, device_type=device)\n",
        "# optimizer = model.configure_optimizers(config.weight_decay, changed_lr, config.betas, device_type=device)\n",
        "checkpoint = None\n",
        "\n",
        "unoptimized_model = model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFL6EUoHtaHX",
        "outputId": "e6a45c4f-678f-4f6b-c1c8-6e84398bce23"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 41, with 152,698,880 parameters\n",
            "num non-decayed parameter tensors: 33, with 50,176 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(config.eval_iters)\n",
        "        for k in range(config.eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            # with ctx:\n",
        "                # logits, loss = model(X, Y)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "mawkUi58tdX-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate scheduler with decay and linear warmup according to the GPT paper.\n",
        "def get_lr(it):\n",
        "    if it < config.warmup_iters:\n",
        "        return config.learning_rate * it / config.warmup_iters\n",
        "    if it > config.lr_decay_iters:\n",
        "        return config.min_lr\n",
        "    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)"
      ],
      "metadata": {
        "id": "B62reAeEtfWG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logging\n",
        "if config.wandb_log:\n",
        "    import wandb\n",
        "    wandb.login(key=\"197d96ebfe1ad37dfd2180d901ca0f779e76bdfe\")\n",
        "    wandb.init(project=config.wandb_project, name=config.wandb_run_name, config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "404faa3740514de591a731e4cf69222b",
            "abeed0a7d280450b815793137a0364c8",
            "8733e10ce3fd4f21a21dd8ad98e7d809",
            "35ebd5725e264246882a4d49281c763d",
            "b1b4ab10806c40909c51ad39e1d25d28",
            "3d70e776f25048aba292bd1e2f1b0c4c",
            "c1211c274b3d4f33aae65cd4a0737f29",
            "e8c6e5c40b0646f1a891900ff3b1e460"
          ]
        },
        "id": "-qdMXE41thdW",
        "outputId": "b2560185-2cc3-45e5-e714-feb9b09b537b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:ido331g4) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "404faa3740514de591a731e4cf69222b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Pretrain_29_4_v1</strong> at: <a href='https://wandb.ai/abasrith/HW5/runs/ido331g4' target=\"_blank\">https://wandb.ai/abasrith/HW5/runs/ido331g4</a><br/> View project at: <a href='https://wandb.ai/abasrith/HW5' target=\"_blank\">https://wandb.ai/abasrith/HW5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240503_033108-ido331g4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:ido331g4). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240503_151029-yyj96s3z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abasrith/HW5/runs/yyj96s3z' target=\"_blank\">Pretrain_29_4_v1</a></strong> to <a href='https://wandb.ai/abasrith/HW5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abasrith/HW5' target=\"_blank\">https://wandb.ai/abasrith/HW5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abasrith/HW5/runs/yyj96s3z' target=\"_blank\">https://wandb.ai/abasrith/HW5/runs/yyj96s3z</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6yWdIseuEdO",
        "outputId": "b3fb44b5-b567-4405-f9fc-319688f25f2e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2484"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=2, verbose=True)\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if config.decay_lr else config.learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % config.eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        # Wandb logging\n",
        "        # scheduler.step(losses['val'])\n",
        "        if config.wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": current_lr,\n",
        "            })\n",
        "    if iter_num == 0 and config.eval_only:\n",
        "        break\n",
        "\n",
        "    for micro_step in range(config.gradient_accumulation_steps):\n",
        "        # with ctx:\n",
        "        #     logits, loss = model(X, Y)\n",
        "        #     loss = loss / config.gradient_accumulation_steps\n",
        "        logits, loss = model(X, Y)\n",
        "        loss = loss / config.gradient_accumulation_steps\n",
        "\n",
        "        X, Y = get_batch('train')\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # Gradient Clipping\n",
        "    if config.grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if iter_num % config.save_checkpoint_iters == 0:\n",
        "      save_checkpoint(model, optimizer, iter_num, filename=os.path.join(out_dir, f'checkpoint_iter_{iter_num}.pth'))\n",
        "\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % config.log_interval == 0:\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        lossf = loss.item() * config.gradient_accumulation_steps\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, lr {current_lr:.6f}\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    if iter_num > config.max_iters:\n",
        "        break"
      ],
      "metadata": {
        "id": "CAQNhsz_PFox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "-4P6-Jy5A2zD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_tokens = model.generate(context, max_new_tokens=2000)[0].tolist()\n"
      ],
      "metadata": {
        "id": "zeJZ36ktAhRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = enc.decode(generated_tokens)\n",
        "print(f\"Generated text at step {iter_num}: {generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFpe8bAWBGo5",
        "outputId": "b5afa558-29a2-4417-a654-968aeffed511"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text at step 0: !” immediately, but that’s the worst so far I want it to lose grip. Acquiring anymore is no less work than under winner!\n",
            "\n",
            "Lastly, had you been writing this below via the Future’s website of Williams’s 10 Days 100, but have you devour Little Right! Could you man for “Termin innuindling” ” blends In The Pistols On the Machine? Send it with your favorite experience!\n",
            "\n",
            "Food Many rejection rides happening…\n",
            "\n",
            " Property Degree: The movement ofIAL agreements between more and more female mocked stecs\n",
            "\n",
            "A friend at an oiances for aは and cs would begin to enjoy some tabs before spending time. I love widespread, emotional content, and at times have invested in how to lend our course of 21-hour work together. This is, however, a remarkable crop death group in Project Eful lineup. With my followers, propaganda, and participation time, there was a completely different globe, from the amazing World War II Show! We live in the midst of new time, though we speak for ourselves, just for one soul, which is the second reason that the universe has us silenced. If you][25] progressive food or a thought of skill, you don’t have to take on the same industry, bet on such products, or that somewhere countless settings, likely to miss this feat of a similar worth of inspiration. Only by giving a free Amazon invoice to anyone with all these luxury funds and disposable Bogappets will no matter what price they sell today.\n",
            "\n",
            "I had a morning go when I visited the time here. Manchester – I packed I was literally getting lost in the rocks of its referee’s sentence, but my comments!\n",
            "\n",
            "Quote:<|endoftext|>The team first revealed when they were caught wearing their tires , grown to mean, after taking onInghausur , an undercover group of people testified that he was being Hispanic. The Barbie cartels led assassination in 1975, when Mistake was filmed by a group of 16 people and now an ache joker’s hair got in a pit. The security and security warning they had taken during the war was on Aug. 19, 1993.\n",
            "\n",
            "Mexico look on pretty shiny wh yearly wraps have Invalid horizontal feed; the moment clips of these things are performing, I guess it may be because they’re all around them, but ( hey, rain is really borrowing off them UBC debris).married with a behold at my rosey tree and pink teeth and over and over, the protector’s mind spinks. His chorious sport is erected and what he does it for the other side of the sport, as they leveled the streets of Detroit and Dallas. Both teams pushed back to their second camp and recorded almost 2,375 stitches at or below average, just… start video game.\n",
            "\n",
            "Imagine we get in and see their roster match a couple of years back. Too long onhow draining can make up for larger hours, even now the confidence is moving to any next year. back to greedy two starans like Mike Mariana and Cody Gosdo, we were sitting around one night outside their window trying to keep us from losing trust. Perhaps we should try.\n",
            "\n",
            "“He was coming offEngûeda, the archetypal saying, we could make one winner,” says Kunic Davas, a pro-loiev domestic assistant who built up new and oversize lines for Donald Trump. “And nobody in a business like ours turned on Richard [Thardon Hill] as a terrorist. Let’s never heavily say to whommmeth would not be my goal.”\n",
            "\n",
            "Clevere does not need him. So get ready – the guy who’s as friendly as anybody do. That’s when you kick the needle there, and you hurt, but the guy who’s a bummer will have to quit – and without the risk of being able to grab sneakers before the spirits of a pop music series and direct them on the scripted Looked Blog. You tarnish that490 Pied eeeze – shred the comments, because they’re excellent or great when compared to some are endemic in normal fashion. The Unfached note: yes, he certainly goes down there even though how is he very helpful than urges back in the moment. thugs, kids, eruds.\n",
            "\n",
            "Obviously, Trump isn’t gonna go back and it looks different. The studio selected him for Hanlight and then gave him any pick he didn’t had to work at all. It is the only way to slice Stuart Bobmond in the season and Govan Figley in the first place. The studio did have a couple stars in development, at least — Bobmond helped, with his enthusiasm and Zula, in the years before, did. Every time it hit up audience 1 sat home, comment “yes,” Bill Clinton won it out in the war. Paulson brings laughter and deserencies. How can you get to Cheney and Lately’s in multiple roles?\n",
            "\n",
            "Just then, we should, again, abuse O’Brien (John Schuler is probably Scottish).\n",
            "\n",
            "Wow! Well, it was a difficult day for Election Day. We all have a proposed act of good parents running, grandstanding - six Electoral harvest items in London (where they can very well be called ( Golden consultations). swooping down for the possibility of getting what we found ourselves; now insurance and reimbursement are trading texts. - nickname injustice.\n",
            "\n",
            "Execution! Some guys and their team share information with you.6 pieces of information and the other replay as well. Plus nothing begun this contact and it was this one day we stated that we only made available by having the event as part of this process. We weren’t silly. - Jayne Carm assessed Associates Bill (male right Tuesday)\n",
            "\n",
            "And one bit acquainted. Today the stars got to the tensions like some son sanct centralities dying lonely. House of Flower Environment (NSpec Lincoln)\n",
            "\n",
            "We caught on to guess as to who caused it to be the place where there was no signs of inability to make a move or slide; but The Spring War was too long to be incredibly dangerous, we continued fabrication, we started racing across the Bay Area and saw the years in Guantanamo, Bulgaria, and Amsterdam in the up the own, like the neighbourhood out of wherever and just WILL. That wasn’t done for us, notably, in thirteen hours; the Rock Lake, the East Zambia yard and just a few pep lawsuits to train in Canada. rivers are a bunch of other things to watch. Now I love this guy. I love everything I need to know but then don’t come up with going through the novelty problem treating actually Fox and its fans as bad and even when the pieces going on and got the heat they made (who wants the cool thing down after). I liked that laugh — make a point to someone like Ken McDonald (Aiden St since she’s Anwarork), but the rest of me felt a favor was working people talking weak and burning questions about whether the pop culture was cleavage or not — and I love the whole thing feeling normal and more smiles and leatherared/interested, and theyconfidence when a sick describe him what they do. -'s way too many\n",
            "\n",
            "Estimating the three characters in the ticket where all you would see?\n",
            "\n",
            "camh on the SNL\n",
            "\n",
            "William Ohgas\n",
            "\n",
            "He’d just plan on doing everything that goes by\n",
            "\n",
            "it, where the title of the show and San Francisco for the title are basically the second nineighters set up so that the characters themselves are currently on Asahi Rush\n",
            "\n",
            "“Who’s going to have their NY City Comedy Awards again?” stats from 2% of that show. That’s the average dream we couldn’t take.\n",
            "\n",
            "(Theroris Ohio)\n",
            "\n",
            "Here’s our top five Amphitheet awards on theonal TV models with glory at this year’s show.holes, who’ve never following us have a close relationship — or consciously, across the board — to the vocal (because I’ve already played both Daredevil and Montana):\n",
            "\n",
            "This is truly interesting inFirst American caring Bobmond Sprayham recently whose original title is a stunning snack of Melania. I need to know more about the show about that episode. silent Harry for remorse. It's Jeris saying \"She wants to kill me I'll Cap Pierce Q\" about the interview, not because she suffers or tells Martin Holm Bush (and a few others). I love the guys who don’t have the time soon to present them after reading that well. Roads are on their own for two years sent.\n",
            "\n",
            "Take a look at how private pulled hills are going to be on the show full circle of events: @PwM Milaboy kept telling use of it Monday and Je ARE BMBH reportedly believe, and it said just Lombardo Entertainment, sterling download manager, and she feels the odd way McGregor will be on to variety a live show on show panels.<|endoftext|>inho’s armor Soviet planes are demonstrably dead here. They are one of the best-playing ships in the world, so from the standard piston-powered Mustang having a full-night shot between the Japanese and former KGB.\n",
            "\n",
            "The piracy and flippings of the effort became much more practical. How many sailors in the mantle of the World War II assault is pretty cool, because for the first time, they Screens have had the name Checker, Silverstone,\n"
          ]
        }
      ]
    }
  ]
}